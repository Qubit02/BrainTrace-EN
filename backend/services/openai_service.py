"""
OpenAI ê¸°ë°˜ ê·¸ë˜í”„ ì¶”ì¶œ/ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤
-------------------------------------

ì´ ëª¨ë“ˆì€ OpenAI APIë¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ë¡œë¶€í„° ë…¸ë“œ/ì—£ì§€(ê·¸ë˜í”„ êµ¬ì„±ìš”ì†Œ)ë¥¼ ì¶”ì¶œí•˜ê³ ,
ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸(ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸)ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ê¸´ í…ìŠ¤íŠ¸(â‰¥2000ì) ì²­í‚¹ ì²˜ë¦¬ í›„ ê° ì²­í¬ì—ì„œ ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œ
- ì¶”ì¶œëœ ë…¸ë“œì˜ descriptionê³¼ ë¬¸ì¥ ì„ë² ë”© ê°„ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ original_sentences ì‚°ì¶œ
- ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ LLM ì§ˆì˜ì‘ë‹µì— í™œìš©
- ë‹µë³€ì˜ ë§¨ ë(JSON ì˜ì—­)ì—ì„œ referenced_nodesë¥¼ íŒŒì‹±í•˜ì—¬ ë…¸ë“œ ì°¸ì¡° ëª©ë¡ì„ ì¶”ì¶œ

í™˜ê²½ ë³€ìˆ˜:
- OPENAI_API_KEY: OpenAI API í˜¸ì¶œì— ì‚¬ìš© (dotenvë¥¼ í†µí•´ ë¡œë“œ)

ì£¼ì˜:
- ë³¸ ëª¨ë“ˆì€ ì™¸ë¶€ API í˜¸ì¶œì„ í¬í•¨í•˜ë¯€ë¡œ, ì¥ì• /ìš”ê¸ˆ/ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- ì„ë² ë”©/ìœ ì‚¬ë„ ê³„ì‚°(threshold)ì€ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ, ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.
"""


import logging
from openai import OpenAI           # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
import json
from .base_ai_service import BaseAIService
from typing import List
from .manual_chunking_sentences import manual_chunking
import numpy as np
import os
from dotenv import load_dotenv  # dotenv ì¶”ê°€
from . import embedding_service
from sklearn.metrics.pairwise import cosine_similarity



# âœ… .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("âŒ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. generate_answerã….env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œì— í™œìš©)
# client = OpenAI(api_key=openai_api_key)
client = OpenAI(api_key=openai_api_key)


class OpenAIService(BaseAIService) :
    """OpenAI APIë¥¼ ì‚¬ìš©í•´ ê·¸ë˜í”„ ì¶”ì¶œ/QAë¥¼ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤ êµ¬í˜„ì²´."""
    def __init__(self, model_name="gpt-4o"):
        # ì¸ìŠ¤í„´ìŠ¤ ì†ì„±ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ í• ë‹¹
        self.client = OpenAI(api_key=openai_api_key)
        self.model_name = model_name  # ëª¨ë¸ëª… ì €ì¥

    def extract_referenced_nodes(self,llm_response: str) -> List[str]:
        """
        LLM ì‘ë‹µ ë¬¸ìì—´ì—ì„œ EOF ë’¤ì˜ JSONì„ íŒŒì‹±í•˜ì—¬ referenced_nodesë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        - 'ë ˆì´ë¸”-ë…¸ë“œ' í˜•ì‹ì¼ ê²½ìš° ë ˆì´ë¸”ê³¼ '-'ì„ ì œê±°í•˜ê³  ë…¸ë“œ ì´ë¦„ë§Œ ë°˜í™˜
        - EOF ì´í›„ JSONì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        parts = llm_response.split("EOF")
        if len(parts) < 2:
            return []

        json_part = parts[-1].strip()
        try:
            payload = json.loads(json_part)
            # payloadê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if isinstance(payload, list):
                return []
            # payloadê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ì—ë§Œ get() í˜¸ì¶œ
            raw_nodes = payload.get("referenced_nodes", [])
            cleaned = [
                node.split("-", 1)[1] if "-" in node else node
                for node in raw_nodes
            ]
            return cleaned
        except json.JSONDecodeError:
            return []
            
    def generate_referenced_nodes(self, llm_response: str, brain_id: str) -> List[str]:
        """
        LLMì´ ìƒì„±í•œ ë‹µë³€ì„ ì„ë² ë”©í•˜ì—¬ ì¼ì • ìœ ì‚¬ë„ ì´ìƒì˜ ë…¸ë“œë“¤ì„ ì°¸ê³ í•œ ë…¸ë“œë¡œ ë°˜í™˜
        
        Args:
            llm_response: LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸
            brain_id: ê²€ìƒ‰í•  brainì˜ ID
        
        Returns:
            ìœ ì‚¬ë„ 0.7 ì´ìƒì¸ ë…¸ë“œë“¤ì˜ name ë¦¬ìŠ¤íŠ¸
        """
        # ì§€ì‹ê·¸ë˜í”„ì— ì •ë³´ê°€ ì—†ë‹¤ëŠ” ì‘ë‹µì¸ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if "ì§€ì‹ê·¸ë˜í”„ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in llm_response:
            logging.info("ì§€ì‹ê·¸ë˜í”„ì— ì •ë³´ ì—†ìŒ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
        
        try:
         
            # LLM ì‘ë‹µì„ ì„ë² ë”©
            response_embedding = embedding_service.encode_text(llm_response)
            
            # ë²¡í„°DBì—ì„œ ìœ ì‚¬í•œ ë…¸ë“œ ê²€ìƒ‰ (threshold 0.7)
            # search_similar_nodesëŠ” (nodes, score_avg) íŠœí”Œì„ ë°˜í™˜
            similar_nodes, avg_score = embedding_service.search_similar_nodes(
                embedding=response_embedding, 
                brain_id=brain_id,
                limit=20,  # limit íŒŒë¼ë¯¸í„° ì‚¬ìš© (top_kê°€ ì•„ë‹˜)
                threshold=0.7  # threshold ëª…ì‹œ
            )
            
            if not similar_nodes:
                logging.info("ë‹µë³€ê³¼ ìœ ì‚¬í•œ ë…¸ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            # ìœ ì‚¬ë„ 0.7 ì´ìƒì¸ ë…¸ë“œë§Œ í•„í„°ë§
            threshold = 0.7
            referenced_nodes = []
            
            for node in similar_nodes:
                # nodeëŠ” {"name": ..., "score": ...} í˜•íƒœ
                if node.get("score", 0) >= threshold:
                    referenced_nodes.append(node["name"])
            
            logging.info(f"âœ… ìœ ì‚¬ë„ {threshold} ì´ìƒì¸ {len(referenced_nodes)}ê°œ ë…¸ë“œë¥¼ ì°¸ì¡° ë…¸ë“œë¡œ ì„ ì •")
            if referenced_nodes:
                # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜ (ë„ˆë¬´ ë§ì€ ë…¸ë“œ ë°©ì§€)
                if len(referenced_nodes) > 10:
                    referenced_nodes = referenced_nodes[:10]
                    logging.info("ìƒìœ„ 10ê°œ ë…¸ë“œë§Œ ì„ íƒ")
                
                logging.info(f"ì„ ì •ëœ ë…¸ë“œ: {', '.join(referenced_nodes[:5])}{'...' if len(referenced_nodes) > 5 else ''}")
            
            return referenced_nodes
            
        except Exception as e:
            logging.error(f"generate_referenced_nodes ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return []

    def _remove_duplicate_nodes(self, nodes: list) -> list:
        """ì¤‘ë³µëœ ë…¸ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        - ë™ì¼ (name, label) ì¡°í•©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê³ , descriptionsëŠ” ë³‘í•©í•©ë‹ˆë‹¤.
        """
        seen = set()
        unique_nodes = []
        for node in nodes:
            node_key = (node["name"], node["label"])
            if node_key not in seen:
                seen.add(node_key)
                unique_nodes.append(node)
            else:
                # ê°™ì€ ì´ë¦„ì˜ ë…¸ë“œê°€ ìˆìœ¼ë©´ descriptionsë§Œ ì¶”ê°€
                for existing_node in unique_nodes:
                    if existing_node["name"] == node["name"] and existing_node["label"] == node["label"]:
                        existing_node["descriptions"].extend(node["descriptions"])
        return unique_nodes

    def _remove_duplicate_edges(self, edges: list) -> list:
        """ì¤‘ë³µëœ ì—£ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (source, target, relation) ë™ì¼ ì‹œ í•˜ë‚˜ë§Œ ìœ ì§€"""
        seen = set()
        unique_edges = []
        for edge in edges:
            edge_key = (edge["source"], edge["target"], edge["relation"])
            if edge_key not in seen:
                seen.add(edge_key)
                unique_edges.append(edge)
        return unique_edges

    def generate_answer(self, schema_text: str, question: str) -> str:
        """
        ì§€ì‹ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ AIë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        logging.info("ğŸš€ OpenAI API í˜¸ì¶œ - ëª¨ë¸: %s", self.model_name)
        
        prompt = (
            "Please answer the question below in natural language, using only the information explicitly provided in the knowledge graph context or that can be reasonably inferred from the relationships. "
            "If relevant information exists, explain it as fully as possible. If the context provides no relevant information, respond with: 'The knowledge graph does not contain this information.'"
            "Knowledge Graph Context Format:\n"
            "1. Relationships: start_name -> relation_label -> end_name\n"
            "2. Nodes: NODE: {node_name} | DESCRIPTION: {desc_str}"
            "Knowledge Graph Context:\n{schema_text}\n"
            "Question: {question}\n"
            "Output:\n[Provide a detailed answer based on the knowledge graph, or write 'The knowledge graph does not contain this information.']"
            )


        try:
        
            response = client.chat.completions.create(
                model=self.model_name,  # ë™ì  ëª¨ë¸ ì„ íƒ
                messages=[{"role": "user", "content": prompt}]
            )
            response = response.choices[0].message.content

            print("response: ", response)
            final_answer = response
            return final_answer
        except Exception as e:
            logging.error("GPT ì‘ë‹µ ì˜¤ë¥˜: %s", str(e))
            raise RuntimeError("GPT ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    

    def generate_schema_text(self, nodes, related_nodes, relationships) -> str:
        """
        ìœ„: start_name -> relation_label -> end_name (í•œ ì¤„ì”©, ì¤‘ë³µ ì œê±°)
        ì•„ë˜: ëª¨ë“  ë…¸ë“œ(ê´€ê³„ ìˆë“  ì—†ë“ ) ì¤‘ë³µ ì—†ì´
            {node_name}: {desc_str}
        desc_strëŠ” original_sentences[].original_sentenceë¥¼ ëª¨ì•„ ê³µë°± ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°
        """
        


        def to_dict(obj):
            """ì…ë ¥ ê°ì²´ë¥¼ dictë¡œ ê´€ìš©ì ìœ¼ë¡œ ë³€í™˜(Neo4j ë ˆì½”ë“œ/ê°ì²´ í˜¸í™˜ìš©)."""
            try:
                if obj is None:
                    return {}
                if hasattr(obj, "items"):
                    return dict(obj.items())
                if isinstance(obj, dict):
                    return obj
            except Exception:
                pass
            return {}

        def normalize_space(s: str) -> str:
            """ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”."""
            return " ".join(str(s).split())

        def filter_node(node_obj):
            """ë…¸ë“œ ë ˆì½”ë“œ/ê°ì²´ì—ì„œ name/label/original_sentencesë§Œ ì¶”ì¶œ/ì •ê·œí™”."""
            d = to_dict(node_obj)
            name = normalize_space(d.get("name", "ì•Œ ìˆ˜ ì—†ìŒ") or "")
            label = normalize_space(d.get("label", "ì•Œ ìˆ˜ ì—†ìŒ") or "")
            original_sentences = d.get("original_sentences", []) or []
            parsed = []
            # ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹± ì‹œë„
            if isinstance(original_sentences, str):
                try:
                    original_sentences = [json.loads(original_sentences)]
                except Exception:
                    original_sentences = []
            # ë¦¬ìŠ¤íŠ¸ ìš”ì†Œë“¤ ì •ê·œí™”
            for item in original_sentences:
                if isinstance(item, str):
                    try:
                        obj = json.loads(item)
                        if isinstance(obj, dict):
                            parsed.append(obj)
                    except Exception:
                        continue
                elif isinstance(item, dict):
                    parsed.append(item)
            return {"name": name, "label": label, "original_sentences": parsed}

        logging.info(
            "generating schema text: %dê°œ ë…¸ë“œ, %dê°œ ê´€ë ¨ ë…¸ë“œ, %dê°œ ê´€ê³„",
            len(nodes) if isinstance(nodes, list) else 0,
            len(related_nodes) if isinstance(related_nodes, list) else 0,
            len(relationships) if isinstance(relationships, list) else 0,
        )

        # 1) ëª¨ë“  ë…¸ë“œ ìˆ˜ì§‘ (name í‚¤ë¡œ í•©ì¹˜ê¸°)
        all_nodes = {}
        if isinstance(nodes, list):
            for n in nodes or []:
                if n is None: continue
                nd = filter_node(n)
                if nd["name"]:
                    all_nodes[nd["name"]] = nd
        if isinstance(related_nodes, list):
            for n in related_nodes or []:
                if n is None: continue
                nd = filter_node(n)
                if nd["name"] and nd["name"] not in all_nodes:
                    all_nodes[nd["name"]] = nd

        # 2) ê´€ê³„ ì¤„ ë§Œë“¤ê¸°: "start -> relation -> end"
        relation_lines = []
        connected_names = set()
        if isinstance(relationships, list):
            for rel in relationships:
                try:
                    if rel is None:
                        continue
                    start_d = to_dict(getattr(rel, "start_node", {}))
                    end_d   = to_dict(getattr(rel, "end_node", {}))
                    start_name = normalize_space(start_d.get("name", "") or "ì•Œ ìˆ˜ ì—†ìŒ")
                    end_name   = normalize_space(end_d.get("name", "") or "ì•Œ ìˆ˜ ì—†ìŒ")

                    # relation label: props.relation ìš°ì„ , ì—†ìœ¼ë©´ type, ì—†ìœ¼ë©´ "ê´€ê³„"
                    try:
                        rel_props = dict(rel)
                    except Exception:
                        rel_props = {}
                    relation_type = getattr(rel, "type", None)
                    relation_label = rel_props.get("relation") or relation_type or "ê´€ê³„"
                    relation_label = normalize_space(relation_label)

                    relation_lines.append(f"{start_name} -> {relation_label} -> {end_name}")
                    connected_names.update([start_name, end_name])
                except Exception as e:
                    logging.exception("ê´€ê³„ ì²˜ë¦¬ ì˜¤ë¥˜: %s", e)
                    continue

        # ê´€ê³„ ì¤‘ë³µ ì œê±° + ì •ë ¬
        relation_lines = sorted(set(relation_lines))

        # 3) ë…¸ë“œ ì„¤ëª… ë§Œë“¤ê¸°: ëª¨ë“  ë…¸ë“œ(ê´€ê³„ ì—¬ë¶€ ë¬´ê´€)
        def extract_desc_str(node_data):
            # original_sentences[].original_sentence ëª¨ì•„ ê³µë°± ì •ë¦¬ + ì¤‘ë³µ ì œê±°
            seen = set()
            pieces = []
            for d in node_data.get("original_sentences", []):
                if isinstance(d, dict):
                    t = normalize_space(d.get("original_sentence", "") or "")
                    if t and t not in seen:
                        seen.add(t)
                        pieces.append(t)
            if not pieces:
                return ""
            s = " ".join(pieces)
            
            return s

        node_lines = []
        for name in sorted(all_nodes.keys()):  # âœ… ê´€ê³„ ì—†ì–´ë„ ëª¨ë“  ë…¸ë“œ ì¶œë ¥
            nd = all_nodes.get(name) or {}
            desc = extract_desc_str(nd)
            if desc:
                node_lines.append(f"{name}: {desc}")
            else:
                node_lines.append(f"{name}:")  # ì„¤ëª…ì´ ë¹„ë©´ ì½œë¡ ë§Œ

        # 4) ìµœì¢… ì¶œë ¥: ìœ„ì—” ê´€ê³„ë“¤, ì•„ë˜ì—” ë…¸ë“œë“¤
        top = "\n".join(relation_lines)
        bottom = "\n".join(node_lines)

        if top and bottom:
            raw_schema_text = f"{top}\n\n{bottom}"
        elif top:
            raw_schema_text = top
        elif bottom:
            raw_schema_text = bottom
        else:
            raw_schema_text = "ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        logging.info("ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (%dì)", len(raw_schema_text))
        return raw_schema_text


    def chat(self, message: str) -> str:
        """
        ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ Ollama LLMì— ë³´ë‚´ê³ ,
        ëª¨ë¸ ì‘ë‹µ ë¬¸ìì—´ë§Œ ë¦¬í„´í•©ë‹ˆë‹¤.
        """
        try:
            resp = self.client.chat.completions.create(
                model=self.model_name,  # ë™ì  ëª¨ë¸ ì„ íƒ
                messages=[{"role": "user", "content": message}],
                stream=False
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"chat ì˜¤ë¥˜: {e}")
            raise
